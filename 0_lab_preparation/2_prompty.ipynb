{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Prompty\n",
    "\n",
    "Microsoft Prompty is a tool designed to help developers create, manage, and evaluate prompts for LLMs more efficiently. It works within the VSCode environment and is especially useful for refining AI interactions in GenAI pplications. <br>\n",
    "Prompty provides a standardized format (using markdown) for defining prompts, making it easier to understand, share, and debug. It allows developers to quickly prototype\n",
    "\n",
    "-   Reference: https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python31013jvsc74a57bd0583258b7c5128adecee42ad609cd1e06db8938097ed3afe48700fab0f5a650ee\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "module_path = \"../../0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt flow service...\n",
      "You can stop the prompt flow service with the following command:'\u001b[1mpf service stop\u001b[0m'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection override\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
    "\n",
    "# override configuration with AzureOpenAIModelConfiguration\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
    "    api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
    "    api_version=\"${env:AZURE_OPENAI_API_VERSION}\",\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0xb076f0d4898f9ec1c0b6e86e78f2eacc\n",
      "https://ai.azure.com/projecttrace/detail/0xb076f0d4898f9ec1c0b6e86e78f2eacc?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0xfba37fdd7412ed501499adf5c3e7ee38\n",
      "https://ai.azure.com/projecttrace/detail/0xfba37fdd7412ed501499adf5c3e7ee38?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0x5179977858e35aaea03a24707dbc6e66\n",
      "https://ai.azure.com/projecttrace/detail/0x5179977858e35aaea03a24707dbc6e66?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0xa7bedc4ea651012fb150f789587a9427\n",
      "https://ai.azure.com/projecttrace/detail/0xa7bedc4ea651012fb150f789587a9427?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0x9b9c57210893bc9088a1d656c4b14c44\n",
      "https://ai.azure.com/projecttrace/detail/0x9b9c57210893bc9088a1d656c4b14c44?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0x963717d266ed93a25f23cfe4a63219dd\n",
      "https://ai.azure.com/projecttrace/detail/0x963717d266ed93a25f23cfe4a63219dd?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0x0026dd41b6b0d3bd1f3dfd3c28142ffb\n",
      "https://ai.azure.com/projecttrace/detail/0x0026dd41b6b0d3bd1f3dfd3c28142ffb?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=0_lab_preparation&uiTraceId=0x5d023ee4fe70d2fa8c56f2a875da3302\n",
      "https://ai.azure.com/projecttrace/detail/0x5d023ee4fe70d2fa8c56f2a875da3302?wsid=/subscriptions/8f94592c-747a-4bb7-ab3e-0b569993c33c/resourceGroups/rg_aistudio/providers/Microsoft.MachineLearningServices/workspaces/demo-aistudio00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tokyo is a city and does not have a capital; it is the capital of Japan.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.core import Prompty\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"./prompty/basic.prompty\", model=override_model)\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of Japan?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'ã‚¢ãƒ«ãƒ‘ã‚¤ãƒ³ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ†ãƒ³ãƒˆã¯ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®å–ã‚Šå¤–ã—å¯èƒ½ãªä»•åˆ‡ã‚Šã‚„ã€å¤šæ•°ã®ãƒ¡ãƒƒã‚·ãƒ¥çª“ã€æ›æ°—ç”¨ã®èª¿ç¯€å¯èƒ½ãªé€šæ°—å£ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚ã¾ãŸã€é˜²æ°´è¨­è¨ˆã‚„ã‚®ã‚¢ãƒ­ãƒ•ãƒˆã‚‚ã‚ã‚Šã€å¿«é©ã•ã¨ä¾¿åˆ©ã•ã‚’å…¼ã­å‚™ãˆã¦ã„ã¾ã™ã€‚è‡ªç„¶ã®ä¸­ã§ã®å¿«é©ãªç©ºé–“ã‚’æä¾›ã™ã‚‹ç´ æ™´ã‚‰ã—ã„ãƒ†ãƒ³ãƒˆã§ã™ï¼ğŸŒ²â›ºï¸',\n",
       " 'joke': 'ã¨ã“ã‚ã§ã€ãƒ†ãƒ³ãƒˆãŒãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œãã¨ãã¯ã„ã¤ã‚‚å¿ƒé…ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã¯ã€èª°ãŒä¸€ç•ªã€Œå¼µã‚Šåˆ‡ã£ã¦ã€ã„ã‚‹ã‹ã¨ã„ã†ã“ã¨ã§ã™ï¼'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Prompty.load(source=\"./prompty/japanese.prompty\", model=override_model)\n",
    "\n",
    "context = \"\"\"\n",
    "ã‚¢ãƒ«ãƒ‘ã‚¤ãƒ³ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ†ãƒ³ãƒˆã«ã¯ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«å–ã‚Šå¤–ã—å¯èƒ½ãªä»•åˆ‡ã‚ŠãŒã‚ã‚Šã¾ã™ã€‚\n",
    "å¤šæ•°ã®ãƒ¡ãƒƒã‚·ãƒ¥çª“ã¨æ›æ°—ç”¨ã®èª¿ç¯€å¯èƒ½ãªé€šæ°—å£ã€ãŠã‚ˆã³\n",
    "é˜²æ°´è¨­è¨ˆãŒç‰¹å¾´ã§ã™ã€‚ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢ã®å¿…éœ€å“ã‚’åç´ã™ã‚‹ãŸã‚ã®ã‚®ã‚¢ãƒ­ãƒ•ãƒˆã‚‚çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™\n",
    "æ©Ÿå™¨ãƒ­ãƒ•ãƒˆãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚è¦ã™ã‚‹ã«ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€å¿«é©ã•ã€\n",
    "ä¾¿åˆ©ã•ã®çµ„ã¿åˆã‚ã›ã¯ã€è‡ªç„¶ã®ä¸­ã§ãã‚Œã‚’ç¬¬äºŒã®å®¶ã«ã—ã¾ã™!\n",
    "\"\"\"\n",
    "result = f(firstName=\"Hyo\", context=context, question=\"ãƒ†ãƒ³ãƒˆã«ã¤ã„ã¦ä½•ã‚’çŸ¥ã‚ŠãŸã„ã§ã™ã‹?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: The capital of Japan is Tokyo.\n",
      "eval_result: {'score': '5', 'explanation': 'Tokyo is the capital of Japan.'}\n"
     ]
    }
   ],
   "source": [
    "flow = Prompty.load(source=\"./prompty/basic.prompty\", model=override_model)\n",
    "eval_flow = Prompty.load(\"./prompty/eval.prompty\", model=override_model)\n",
    "\n",
    "question = \"What is the capital of Japan?\"\n",
    "ground_truth = \"Tokyo\"\n",
    "\n",
    "result = flow(question=question)\n",
    "eval_result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
    "\n",
    "print(f\"result: {result}\")\n",
    "print(f\"eval_result: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœˆã¯åœ°çƒã‹ã‚‰ç´„384,400ã‚­ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«ã®è·é›¢ã«ã‚ã‚Šã¾ã™ã€‚ã“ã®è·é›¢ã¯ã€åœ°çƒã¨æœˆã®é–“ã®å¹³å‡çš„ãªè·é›¢ã§ã‚ã‚Šã€é‡åŠ›ã®å½±éŸ¿ã‚„è»Œé“ã®å¤‰å‹•ã«ã‚ˆã£ã¦è‹¥å¹²ã®å¤‰åŒ–ãŒã‚ã‚Šã¾ã™ã€‚ğŸŒŒ\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = \"Japanese\"\n",
    "f = Prompty.load(\"./prompty/chat.prompty\", model=override_model)\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a factual chatbot that is also sarcastic.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}\n",
    "]\n",
    "question = f\"Can you speak more scientifically in {LANGUAGE}?\"\n",
    "result = f(chat_history=chat_history, question=question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
